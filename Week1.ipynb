{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Me-lab-c/AICTE_internship_garbage_classification/blob/main/Copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Importing NumPy for numerical operations and array manipulations\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib for plotting graphs and visualizations\n",
        "import seaborn as sns  # Importing Seaborn for statistical data visualization, built on top of Matplotlib\n",
        "import tensorflow as tf  # Importing TensorFlow for building and training machine learning models\n",
        "from tensorflow import keras  # Importing Keras, a high-level API for TensorFlow, to simplify model building\n",
        "from tensorflow.keras import Layer  # Importing Layer class for creating custom layers in Keras\n",
        "from tensorflow.keras.models import Sequential  # Importing Sequential model for building neural networks layer-by-layer\n",
        "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks  # Importing various modules for layers, optimizers, and callbacks in Keras\n",
        "from sklearn.utils.class_weight import compute_class_weight  # Importing function to compute class weights for imbalanced datasets\n",
        "from tensorflow.keras.applications import EfficientNetV2B2  # Importing EfficientNetV2S model for transfer learning\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Importing functions to evaluate model performance\n",
        "import gradio as gr  # Importing Gradio for creating interactive web interfaces for machine learning models\n",
        "from tensorflow.keras.utils import image_dataset_from_directory  # Importing function to load image data from a directory\n",
        "from tensorflow.keras.layers import Dense  # Importing Dense layer for fully connected layers in neural networks"
      ],
      "metadata": {
        "id": "Mnv4d-FuzuKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for directly asseces dataset fron kaagle\n",
        "\n",
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "import os # Import the os module"
      ],
      "metadata": {
        "id": "s_MnbXFN5fVJ",
        "outputId": "8137f4ae-d320-4d4f-f1bb-7d7367f7f4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_kaggle_dataset_url' with the URL of the Kaggle dataset\n",
        "dataset_url = 'https://www.kaggle.com/datasets/mostafaabla/garbage-classification'\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "id": "WhGJoK4k7IkE",
        "outputId": "497bd658-4508-4a1a-d5cc-acf872af22ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./garbage-classification\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the downloaded dataset directory\n",
        "dataset_dir = '/content/garbage-classification'"
      ],
      "metadata": {
        "id": "q1RK0W7Z7zZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size and batch size\n",
        "img_height = 260\n",
        "img_width = 260\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "3T14qFYY73jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training, validation, and test datasets\n",
        "train_ds = image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ZprrP2aN78CJ",
        "outputId": "455e5453-4ca8-417d-f0bd-2a81e42e33c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15515 files belonging to 1 classes.\n",
            "Using 12412 files for training.\n",
            "Found 15515 files belonging to 1 classes.\n",
            "Using 3103 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of classes\n",
        "num_classes = len(train_ds.class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Build the model\n",
        "base_model = EfficientNetV2B2(input_shape=(img_height, img_width, 3),\n",
        "                              include_top=False,\n",
        "                              weights='imagenet')"
      ],
      "metadata": {
        "id": "DZjDt3gx8BRM",
        "outputId": "b5325eb7-9d10-443e-b2ab-8023958a446c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax') # Use softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model (example using Adam optimizer)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # Use this if your labels are integers\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1sNkmjAv8Dds"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
